{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sigmoid function defination\n",
    "def sigmoid(z):\n",
    "    s=1.0/(1+np.exp(-z))\n",
    "    return s\n",
    "\n",
    "#softmax function for multiclass classification\n",
    "def softmax(z):\n",
    "    s=np.exp(z)/np.sum(np.exp(z),axis=0)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Accuracy Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy funciton\n",
    "def accuracy(y,pred):\n",
    "    count=0.0\n",
    "    for i in range(0,y.shape[1]):\n",
    "        if(y[0][i]==pred[i]):\n",
    "            count=count+1\n",
    "    print (count)\n",
    "    return count*100/y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing the layer sizes, assuming the hidden layer has 4 units\n",
    "def layer_sizes(X,Y,n_h):\n",
    "    n_x\t= X.shape[0]\n",
    "    n_y = Y.shape[0]\n",
    "    n_h = n_h\n",
    "    return (n_x,n_h,n_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing weights and biases for layer 1 and layer 2\n",
    "def initialize_paramteres(n_x,n_h,n_y):\n",
    "    W1=np.random.randn(n_h,n_x)*0.01\n",
    "    b1=np.zeros((n_h,1))\n",
    "    W2=np.random.randn(n_y,n_h)*0.01\n",
    "    b2=np.zeros((n_y,1))\n",
    "\n",
    "    assert(W1.shape==(n_h,n_x))\n",
    "    assert(b1.shape==(n_h,1))\n",
    "    assert(W2.shape==(n_y,n_h))\n",
    "    assert(b2.shape==(n_y,1))\n",
    "\n",
    "    parameters={\"W1\":W1,\n",
    "                \"b1\":b1,\n",
    "                \"W2\":W2,\n",
    "                \"b2\":b2}\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#forward propagation to compute activation values. 13 is the number of classes \n",
    "def forward_propagation\t(X,parameters):\n",
    "    W1=parameters[\"W1\"]\n",
    "    b1=parameters[\"b1\"]\n",
    "    W2=parameters[\"W2\"]\n",
    "    b2=parameters[\"b2\"]\n",
    "    \n",
    "    Z1=np.dot(W1,X)+b1\n",
    "    A1=np.tanh(Z1)\n",
    "    Z2=np.dot(W2,A1)+b2\n",
    "    A2=softmax(Z2)\n",
    "\n",
    "\n",
    "    assert(A2.shape==(13,X.shape[1]))\n",
    "\n",
    "    cache={\"Z1\":Z1,\n",
    "            \"A1\":A1,\n",
    "            \"Z2\":Z2,\n",
    "            \"A2\":A2}\n",
    "    return A2,cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computes cost based on softmax function\n",
    "def compute_cost(A2,Y,parameters):\n",
    "    m=Y.shape[1]\n",
    "    logprobs=(np.multiply(np.log(A2),Y))\n",
    "    cost=-np.sum(logprobs)/m\n",
    "    cost=float(cost)\n",
    "    assert(isinstance(cost,float))\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computes gradients for learning\n",
    "def backward_propagation(parameters,cache,X,Y):\n",
    "    m=X.shape[1]\n",
    "    W1=parameters[\"W1\"]\n",
    "    W2=parameters[\"W2\"]\n",
    "\n",
    "    A1=cache[\"A1\"]\n",
    "    A2=cache[\"A2\"]\n",
    "\n",
    "    dZ2=A2-Y\n",
    "    dW2=np.dot(dZ2,A1.T)/m\n",
    "    db2=np.sum(dZ2,axis=1,keepdims=True)/m\n",
    "    dZ1=np.dot(W2.T,dZ2)*(1-np.power(A1,2))\n",
    "    dW1=np.dot(dZ1,X.T)/m\n",
    "    db1=np.sum(dZ1,axis=1,keepdims=True)/m\n",
    "\n",
    "    grads={\"dW1\":dW1,\n",
    "           \"db1\":db1,\n",
    "           \"dW2\":dW2,\n",
    "           \"db2\":db2}\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update parameters based on learning rate and gradients dW and db\n",
    "def update_paramters(parameters,grads,learning_rate):\n",
    "    W1=parameters[\"W1\"]\n",
    "    b1=parameters[\"b1\"]\n",
    "    W2=parameters[\"W2\"]\n",
    "    b2=parameters[\"b2\"]\n",
    "\n",
    "    dW1=grads[\"dW1\"]\n",
    "    db1=grads[\"db1\"]\n",
    "    dW2=grads[\"dW2\"]\n",
    "    db2=grads[\"db2\"]\n",
    "    \n",
    "    W1=W1-learning_rate*dW1\n",
    "    b1=b1-learning_rate*db1\n",
    "    W2=W2-learning_rate*dW2\n",
    "    b2=b2-learning_rate*db2\n",
    "\n",
    "    parameters={\"W1\":W1,\n",
    "                \"b1\":b1,\n",
    "                \"W2\":W2,\n",
    "                \"b2\":b2}\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model for NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model(X,Y,n_h,num_iterations=1000, print_cost=False):\n",
    "    n_x,n_h,n_y=layer_sizes(X,Y,n_h)\n",
    "    print(\"size of imput layer is n_x =\"+str(n_x))\n",
    "    print(\"size of hidden layer is n_xh =\"+str(n_h))\n",
    "    print(\"size of output layer is n_y =\"+str(n_y))\n",
    "\n",
    "    parameters= initialize_paramteres(n_x,n_h,n_y)\n",
    "\n",
    "\n",
    "    for i in range(0,num_iterations):\n",
    "\n",
    "        A2,cache=forward_propagation(X,parameters)\n",
    "        cost =compute_cost(A2,Y,parameters)\n",
    "        grads=backward_propagation(parameters,cache,X,Y)\n",
    "        parameters=update_paramters(parameters,grads,0.001)\n",
    "        if print_cost and i%100==0:\n",
    "            print(\"Cost after iteration %i:%f\" %(i,cost))\n",
    "            #print(A2.shape)\n",
    "            #print(A2)\n",
    "            #print(np.sum(A2,axis=0).shape)\n",
    "            #print(np.sum(A2, axis=0))\n",
    "\n",
    "\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predict Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(parameters, X):\n",
    "    A2, cache = forward_propagation(X,parameters)\n",
    "    print(np.transpose(A2))\n",
    "    predictions=np.zeros(A2.shape[1])\n",
    "    predictions=np.argmax(A2,axis=0)+1\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader=csv.reader(open(\"reduced_features.csv\",\"r\"),delimiter=\",\")\n",
    "X=list(reader)\n",
    "X=np.array(X)\n",
    "X=X.astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader=csv.reader(open(\"target_output.csv\",\"r\"),delimiter=\",\")\n",
    "Y=list(reader)\n",
    "Y=np.array(Y)\n",
    "Y=Y.astype(np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc=StandardScaler()\n",
    "X=sc.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test= train_test_split(X,Y,test_size=0.1, random_state=0)\n",
    "\n",
    "\n",
    "Y_train=np.transpose(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encoding for multiclass\n",
    "one_hot_encoded=list()\n",
    "for value in range (0,Y_train.shape[1]):\n",
    "    out=list()\n",
    "    out=[0 for i in range(13)]\n",
    "    out[Y_train[0][value]-1]=1\n",
    "    one_hot_encoded.append(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x =(175, 406)\n",
      "shape of y =(13, 406)\n"
     ]
    }
   ],
   "source": [
    "Y_train=one_hot_encoded\n",
    "Y_train=np.array(Y_train)\n",
    "\n",
    "\n",
    "X_train=np.transpose(X_train)\n",
    "Y_train=np.transpose(Y_train)\n",
    "\n",
    "X_test=np.transpose(X_test)\n",
    "Y_test=np.transpose(Y_test)\n",
    "\n",
    "\n",
    "print(\"shape of x =\"+str(X_train.shape))\n",
    "print(\"shape of y =\"+str(Y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of imput layer is n_x =175\n",
      "size of hidden layer is n_xh =70\n",
      "size of output layer is n_y =13\n",
      "Cost after iteration 0:2.566388\n",
      "Cost after iteration 100:2.537709\n",
      "Cost after iteration 200:2.509446\n",
      "Cost after iteration 300:2.481525\n",
      "Cost after iteration 400:2.453867\n",
      "Cost after iteration 500:2.426394\n",
      "Cost after iteration 600:2.399020\n",
      "Cost after iteration 700:2.371652\n",
      "Cost after iteration 800:2.344188\n",
      "Cost after iteration 900:2.316515\n",
      "Cost after iteration 1000:2.288512\n",
      "Cost after iteration 1100:2.260046\n",
      "Cost after iteration 1200:2.230979\n",
      "Cost after iteration 1300:2.201174\n",
      "Cost after iteration 1400:2.170498\n",
      "Cost after iteration 1500:2.138837\n",
      "Cost after iteration 1600:2.106106\n",
      "Cost after iteration 1700:2.072267\n",
      "Cost after iteration 1800:2.037339\n",
      "Cost after iteration 1900:2.001416\n",
      "Cost after iteration 2000:1.964667\n",
      "Cost after iteration 2100:1.927336\n",
      "Cost after iteration 2200:1.889732\n",
      "Cost after iteration 2300:1.852201\n",
      "Cost after iteration 2400:1.815100\n",
      "Cost after iteration 2500:1.778760\n",
      "Cost after iteration 2600:1.743465\n",
      "Cost after iteration 2700:1.709432\n",
      "Cost after iteration 2800:1.676800\n",
      "Cost after iteration 2900:1.645639\n",
      "Cost after iteration 3000:1.615957\n",
      "Cost after iteration 3100:1.587714\n",
      "Cost after iteration 3200:1.560835\n",
      "Cost after iteration 3300:1.535226\n",
      "Cost after iteration 3400:1.510782\n",
      "Cost after iteration 3500:1.487398\n",
      "Cost after iteration 3600:1.464975\n",
      "Cost after iteration 3700:1.443424\n",
      "Cost after iteration 3800:1.422665\n",
      "Cost after iteration 3900:1.402633\n",
      "Cost after iteration 4000:1.383276\n",
      "Cost after iteration 4100:1.364552\n",
      "Cost after iteration 4200:1.346428\n",
      "Cost after iteration 4300:1.328881\n",
      "Cost after iteration 4400:1.311892\n",
      "Cost after iteration 4500:1.295449\n",
      "Cost after iteration 4600:1.279542\n",
      "Cost after iteration 4700:1.264162\n",
      "Cost after iteration 4800:1.249301\n",
      "Cost after iteration 4900:1.234950\n",
      "Cost after iteration 5000:1.221101\n",
      "Cost after iteration 5100:1.207742\n",
      "Cost after iteration 5200:1.194861\n",
      "Cost after iteration 5300:1.182444\n",
      "Cost after iteration 5400:1.170476\n",
      "Cost after iteration 5500:1.158941\n",
      "Cost after iteration 5600:1.147819\n",
      "Cost after iteration 5700:1.137093\n",
      "Cost after iteration 5800:1.126743\n",
      "Cost after iteration 5900:1.116750\n",
      "Cost after iteration 6000:1.107094\n",
      "Cost after iteration 6100:1.097756\n",
      "Cost after iteration 6200:1.088718\n",
      "Cost after iteration 6300:1.079961\n",
      "Cost after iteration 6400:1.071467\n",
      "Cost after iteration 6500:1.063220\n",
      "Cost after iteration 6600:1.055203\n",
      "Cost after iteration 6700:1.047402\n",
      "Cost after iteration 6800:1.039801\n",
      "Cost after iteration 6900:1.032388\n",
      "Cost after iteration 7000:1.025150\n",
      "Cost after iteration 7100:1.018075\n",
      "Cost after iteration 7200:1.011152\n",
      "Cost after iteration 7300:1.004372\n",
      "Cost after iteration 7400:0.997724\n",
      "Cost after iteration 7500:0.991201\n",
      "Cost after iteration 7600:0.984794\n",
      "Cost after iteration 7700:0.978496\n",
      "Cost after iteration 7800:0.972300\n",
      "Cost after iteration 7900:0.966200\n",
      "Cost after iteration 8000:0.960191\n",
      "Cost after iteration 8100:0.954267\n",
      "Cost after iteration 8200:0.948423\n",
      "Cost after iteration 8300:0.942655\n",
      "Cost after iteration 8400:0.936960\n",
      "Cost after iteration 8500:0.931333\n",
      "Cost after iteration 8600:0.925772\n",
      "Cost after iteration 8700:0.920273\n",
      "Cost after iteration 8800:0.914834\n",
      "Cost after iteration 8900:0.909452\n",
      "Cost after iteration 9000:0.904125\n",
      "Cost after iteration 9100:0.898851\n",
      "Cost after iteration 9200:0.893629\n",
      "Cost after iteration 9300:0.888456\n",
      "Cost after iteration 9400:0.883332\n",
      "Cost after iteration 9500:0.878254\n",
      "Cost after iteration 9600:0.873222\n",
      "Cost after iteration 9700:0.868235\n",
      "Cost after iteration 9800:0.863292\n",
      "Cost after iteration 9900:0.858391\n"
     ]
    }
   ],
   "source": [
    "#running the model for given number of iterations\n",
    "parameters = nn_model(X_train, Y_train, 70, num_iterations=10000, print_cost=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.09410581e-01 1.38080382e-02 1.01261536e-02 1.27751858e-02\n",
      "  2.49668517e-02 2.80416420e-02 1.37552656e-02 1.29094373e-02\n",
      "  5.79233430e-03 2.36154303e-02 1.22961195e-02 1.18280282e-02\n",
      "  2.06749327e-02]\n",
      " [2.84086777e-01 2.81759361e-01 2.34719148e-02 5.93372469e-02\n",
      "  2.63517948e-02 5.74678167e-02 3.19628469e-02 3.53324266e-02\n",
      "  4.72443297e-02 2.67808138e-02 3.25057730e-02 4.50652120e-02\n",
      "  4.86336865e-02]\n",
      " [6.86383484e-03 1.92983441e-03 1.37627516e-02 4.04929499e-03\n",
      "  1.37760340e-02 1.56843847e-02 1.00604822e-02 9.89353234e-03\n",
      "  8.30249716e-03 8.77812285e-01 1.34410684e-02 8.51602640e-03\n",
      "  1.59079740e-02]\n",
      " [9.04091181e-01 3.55544252e-02 2.13744248e-03 6.20637619e-03\n",
      "  3.45654647e-03 1.62599286e-02 4.77424923e-03 4.68977962e-03\n",
      "  5.52855334e-03 6.42372710e-04 3.94729414e-03 5.19397631e-03\n",
      "  7.51787485e-03]\n",
      " [4.91854646e-01 1.45293718e-01 2.08808327e-02 8.24896971e-02\n",
      "  4.10557547e-02 3.62912597e-02 2.63413705e-02 2.77111800e-02\n",
      "  2.24661570e-02 1.10521276e-02 2.48225377e-02 3.45505562e-02\n",
      "  3.51901625e-02]\n",
      " [7.01447087e-01 6.03014831e-03 1.34287039e-02 1.17264998e-02\n",
      "  3.27689424e-02 4.07259371e-02 1.94667406e-02 1.71629104e-02\n",
      "  8.77526942e-03 8.37599044e-02 1.95450411e-02 1.30401826e-02\n",
      "  3.21226332e-02]\n",
      " [5.11565686e-02 2.57872244e-01 4.77454064e-02 1.63299529e-01\n",
      "  6.47888924e-02 3.60074361e-02 3.80980836e-02 4.74579521e-02\n",
      "  3.38933362e-02 1.11501984e-01 4.12147954e-02 6.21636427e-02\n",
      "  4.48001296e-02]\n",
      " [3.75934082e-03 6.24661559e-01 1.86026823e-02 1.25666846e-02\n",
      "  1.54592186e-03 1.19435774e-02 7.69934394e-03 1.00863070e-02\n",
      "  2.67860152e-01 1.62400276e-03 8.06749598e-03 2.09299308e-02\n",
      "  1.06530014e-02]\n",
      " [6.90733063e-01 6.51506721e-02 7.22991647e-03 3.68973832e-02\n",
      "  2.27315776e-02 3.87302430e-02 1.88032152e-02 1.92821283e-02\n",
      "  1.11788452e-02 2.01272033e-02 1.92284056e-02 2.00938371e-02\n",
      "  2.98135098e-02]\n",
      " [7.99877842e-03 1.16647426e-02 7.38958570e-01 2.97366639e-02\n",
      "  4.85257161e-02 1.37100365e-02 2.00173800e-02 1.87334660e-02\n",
      "  3.22182880e-02 1.71598101e-02 1.61650488e-02 2.98073136e-02\n",
      "  1.53041864e-02]\n",
      " [1.52881294e-02 2.94437370e-02 6.16519556e-01 2.65708227e-02\n",
      "  5.33313491e-02 2.02665865e-02 2.77395481e-02 2.67793764e-02\n",
      "  7.31542786e-02 2.02060173e-02 2.48229569e-02 4.30621195e-02\n",
      "  2.28155221e-02]\n",
      " [4.79468315e-01 1.77919208e-01 1.03679933e-02 6.62290232e-02\n",
      "  4.79475790e-02 2.54068711e-02 2.48781327e-02 2.63928633e-02\n",
      "  1.51336449e-02 2.79607489e-02 2.78886407e-02 3.21365396e-02\n",
      "  3.82704404e-02]\n",
      " [8.56057521e-01 2.98335472e-02 1.07013608e-02 9.30885289e-03\n",
      "  8.47592053e-03 2.74470071e-02 8.99801927e-03 9.11446933e-03\n",
      "  8.81065635e-03 2.88151336e-03 6.53270659e-03 9.64854817e-03\n",
      "  1.21898773e-02]\n",
      " [8.19052737e-01 9.52073766e-03 5.97794502e-03 9.70277883e-03\n",
      "  1.67593034e-02 3.58871339e-02 1.31273099e-02 1.18774518e-02\n",
      "  6.31466857e-03 2.61956247e-02 1.31160154e-02 9.88169385e-03\n",
      "  2.25866002e-02]\n",
      " [9.62124834e-02 4.01236722e-01 8.49838706e-02 3.59008987e-02\n",
      "  1.44214111e-02 7.79715619e-02 2.73987818e-02 3.34623099e-02\n",
      "  1.05723566e-01 7.42297810e-03 2.26239871e-02 6.24079033e-02\n",
      "  3.02335263e-02]\n",
      " [3.00813267e-01 2.55482885e-02 2.42491997e-02 3.44133531e-02\n",
      "  7.81825891e-02 3.44615347e-02 3.48317354e-02 3.32719184e-02\n",
      "  1.79832699e-02 2.87284737e-01 4.23895423e-02 2.76673927e-02\n",
      "  5.89031721e-02]\n",
      " [7.03895785e-01 5.19811253e-02 1.12001721e-02 5.25005336e-02\n",
      "  3.34183039e-02 2.79813531e-02 1.76765329e-02 1.80272629e-02\n",
      "  7.84588887e-03 1.46779169e-02 1.63053816e-02 1.96034409e-02\n",
      "  2.48863028e-02]\n",
      " [7.47341864e-01 4.84138568e-02 8.91013121e-03 1.35187861e-02\n",
      "  8.09627223e-03 6.89106677e-02 1.43545952e-02 1.48461177e-02\n",
      "  1.72287922e-02 7.55381203e-03 1.21592466e-02 1.64498464e-02\n",
      "  2.22160123e-02]\n",
      " [8.03040461e-02 3.96184770e-01 5.87399128e-02 7.58396346e-02\n",
      "  6.03620557e-02 2.56447998e-02 3.39638968e-02 3.95296041e-02\n",
      "  4.15639890e-02 5.05918255e-02 3.55031086e-02 6.17609499e-02\n",
      "  4.00114076e-02]\n",
      " [4.30546367e-03 8.03538456e-02 5.51080574e-01 7.27550486e-02\n",
      "  4.52012265e-02 1.65903875e-02 2.47763735e-02 2.93585497e-02\n",
      "  5.98267362e-02 2.17630765e-02 2.18319701e-02 5.40519513e-02\n",
      "  1.81047968e-02]\n",
      " [5.54795708e-01 1.35195803e-01 1.02902587e-02 7.55134804e-02\n",
      "  2.42629347e-02 3.14255955e-02 2.23624206e-02 2.15819118e-02\n",
      "  2.95489045e-02 5.58724580e-03 2.43524901e-02 2.61790923e-02\n",
      "  3.89041552e-02]\n",
      " [6.02493284e-01 2.10929123e-01 1.29713352e-02 2.47146608e-02\n",
      "  1.27467448e-02 3.40093173e-02 1.38858051e-02 1.59259787e-02\n",
      "  1.70623310e-02 3.55123037e-03 1.08217260e-02 2.26355996e-02\n",
      "  1.82528637e-02]\n",
      " [6.98044691e-01 1.40336634e-01 6.03394877e-03 2.15234611e-02\n",
      "  7.46408292e-03 3.85815432e-02 1.18100616e-02 1.29357336e-02\n",
      "  1.65419288e-02 3.10690047e-03 9.96272730e-03 1.57288320e-02\n",
      "  1.79294550e-02]\n",
      " [6.26832110e-01 2.18894185e-03 1.54773786e-02 9.22114601e-03\n",
      "  3.63475781e-02 4.78352822e-02 2.48304489e-02 1.87978799e-02\n",
      "  1.46520603e-02 1.13145209e-01 3.07911971e-02 1.19526850e-02\n",
      "  4.79280828e-02]\n",
      " [9.47787333e-04 1.21270881e-03 3.74335569e-03 1.09290719e-02\n",
      "  3.05749404e-02 1.22869911e-03 3.94343193e-03 4.17425514e-03\n",
      "  7.37199279e-04 9.27177402e-01 6.35181631e-03 3.65929057e-03\n",
      "  5.32004139e-03]\n",
      " [8.66617290e-01 2.65023784e-02 6.23722594e-03 1.04979655e-02\n",
      "  1.04857481e-02 2.36016785e-02 8.69712483e-03 8.68180057e-03\n",
      "  5.50354096e-03 4.36781452e-03 7.00864706e-03 9.10179004e-03\n",
      "  1.26969958e-02]\n",
      " [7.91483096e-01 2.18992683e-02 9.44234647e-03 2.21812554e-02\n",
      "  1.58720640e-02 3.92017421e-02 1.47961543e-02 1.42779676e-02\n",
      "  1.12528119e-02 1.02725331e-02 1.30110231e-02 1.27861626e-02\n",
      "  2.35235750e-02]\n",
      " [6.60075630e-01 8.01703401e-02 1.51295956e-02 3.95027769e-02\n",
      "  2.81413876e-02 4.29498968e-02 1.95913335e-02 2.09820266e-02\n",
      "  1.14201137e-02 1.43065399e-02 1.65398353e-02 2.54298077e-02\n",
      "  2.57607162e-02]\n",
      " [8.38847492e-03 5.64314783e-03 1.76814969e-02 9.26172531e-03\n",
      "  2.77585821e-02 1.09341246e-02 1.26541354e-02 1.33323759e-02\n",
      "  8.12508552e-03 8.38558206e-01 1.69407598e-02 1.19547518e-02\n",
      "  1.87671339e-02]\n",
      " [8.77233028e-01 9.01781097e-03 3.39405897e-03 1.62538184e-02\n",
      "  1.15132938e-02 1.54330360e-02 9.83855547e-03 7.79776169e-03\n",
      "  9.88874232e-03 3.11272344e-03 1.07347646e-02 6.99468569e-03\n",
      "  1.87877206e-02]\n",
      " [4.34471063e-01 5.69195557e-02 3.30356416e-02 2.74298964e-02\n",
      "  5.22640593e-02 5.88930608e-02 3.65063368e-02 3.59346060e-02\n",
      "  2.75450118e-02 1.09782469e-01 3.68376333e-02 3.70879317e-02\n",
      "  5.32927345e-02]\n",
      " [8.30851432e-01 3.04873412e-03 5.20552697e-03 4.25944354e-03\n",
      "  1.39375892e-02 2.76991291e-02 1.01966912e-02 8.85040777e-03\n",
      "  4.04828781e-03 5.76953234e-02 9.90170599e-03 5.78942254e-03\n",
      "  1.85163060e-02]\n",
      " [5.99595112e-01 4.70111983e-02 7.75885260e-03 6.35185159e-02\n",
      "  3.29603716e-02 4.14997482e-02 2.69671726e-02 2.47825276e-02\n",
      "  2.16513055e-02 2.60568711e-02 3.33950300e-02 2.49902278e-02\n",
      "  4.98130669e-02]\n",
      " [5.93439973e-01 5.91911760e-04 7.07568762e-03 3.40668843e-03\n",
      "  2.14615408e-02 3.57841243e-02 1.38464480e-02 1.05834879e-02\n",
      "  5.25003499e-03 2.59518925e-01 1.51890123e-02 5.21311005e-03\n",
      "  2.86390562e-02]\n",
      " [8.69802974e-02 3.44885320e-01 1.84375258e-02 3.21798164e-01\n",
      "  3.71582622e-02 2.02496883e-02 2.27924686e-02 2.65070223e-02\n",
      "  2.00231358e-02 1.05498108e-02 2.40099840e-02 3.83241402e-02\n",
      "  2.82841807e-02]\n",
      " [7.90878641e-01 3.37529892e-03 1.04676176e-02 8.83973215e-03\n",
      "  2.01899289e-02 3.95347836e-02 1.64486917e-02 1.28965142e-02\n",
      "  1.16630745e-02 2.98234464e-02 1.65184644e-02 9.35949752e-03\n",
      "  3.00043097e-02]\n",
      " [3.55544588e-01 2.40620656e-02 3.54910410e-02 3.46002831e-02\n",
      "  4.72233258e-02 7.58367466e-02 4.20921900e-02 3.92572350e-02\n",
      "  3.82989317e-02 1.59380498e-01 4.57920148e-02 3.35117370e-02\n",
      "  6.89093434e-02]\n",
      " [3.10722893e-03 2.12495503e-03 2.05445437e-02 7.12879188e-03\n",
      "  4.32186379e-02 4.14771855e-03 1.11418947e-02 1.09053253e-02\n",
      "  8.17817830e-03 8.46133508e-01 1.82411061e-02 8.53521622e-03\n",
      "  1.65928950e-02]\n",
      " [4.96954538e-04 9.50808793e-02 4.97012925e-01 5.65113220e-02\n",
      "  2.78329159e-02 1.26330532e-02 2.18269141e-02 2.84096671e-02\n",
      "  1.02233670e-01 6.72574117e-02 2.34844479e-02 4.95506849e-02\n",
      "  1.76691546e-02]\n",
      " [2.16101398e-02 6.46123092e-02 4.89817038e-01 3.51844441e-02\n",
      "  2.93271305e-02 3.87566475e-02 3.06749896e-02 3.13879455e-02\n",
      "  1.42256544e-01 9.02791213e-03 2.58455303e-02 5.49854824e-02\n",
      "  2.65138865e-02]\n",
      " [2.58313607e-02 1.45455607e-03 1.29830792e-02 5.08374805e-03\n",
      "  2.54033380e-02 1.59415797e-02 1.34153294e-02 1.17109908e-02\n",
      "  7.81753872e-03 8.30325861e-01 1.83870920e-02 8.25114754e-03\n",
      "  2.33943788e-02]\n",
      " [6.53559380e-02 5.92460616e-01 2.05589489e-02 5.44611660e-02\n",
      "  2.02921825e-02 3.22963777e-02 2.19368809e-02 2.77510570e-02\n",
      "  4.25400352e-02 1.87188257e-02 2.35127935e-02 5.18909871e-02\n",
      "  2.82241920e-02]\n",
      " [7.83408655e-01 3.16782537e-03 1.02881434e-02 3.31838233e-03\n",
      "  6.59259669e-03 8.49120960e-02 1.31846513e-02 1.10400154e-02\n",
      "  1.85333859e-02 2.06289931e-02 1.18300426e-02 8.78979280e-03\n",
      "  2.43054202e-02]\n",
      " [9.45649033e-01 1.20029164e-03 1.47132845e-03 1.49485452e-03\n",
      "  4.09099117e-03 1.70087384e-02 3.89779916e-03 3.17740513e-03\n",
      "  1.59221147e-03 7.09352833e-03 3.60007176e-03 2.15064841e-03\n",
      "  7.57309877e-03]\n",
      " [8.64433905e-01 2.02768054e-02 3.43411490e-03 1.00137245e-02\n",
      "  8.66900729e-03 2.69217380e-02 9.52053180e-03 8.79445184e-03\n",
      "  7.75373276e-03 5.54576783e-03 9.29838481e-03 8.35408046e-03\n",
      "  1.69837554e-02]\n",
      " [4.80413955e-03 1.20575520e-01 2.18493317e-01 4.22652408e-02\n",
      "  5.31611683e-02 1.99007152e-02 3.68377975e-02 4.26446301e-02\n",
      "  1.28383562e-01 1.86049308e-01 4.43440369e-02 6.29691947e-02\n",
      "  3.95713708e-02]]\n",
      "predictions = [ 1  1 10  1  1  1  2  2  1  3  3  1  1  1  2  1  1  1  2  3  1  1  1  1\n",
      " 10  1  1  1 10  1  1  1  1  1  2  1  1 10  3  3 10  2  1  1  1  3]\n"
     ]
    }
   ],
   "source": [
    "#using the paramters values to predict future values i.e values of testing set\n",
    "predictions=predict(parameters,X_test)\n",
    "\n",
    "#predictions for testing set\n",
    "print(\"predictions = \"+str(predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.0\n",
      "Accuracy = 58.69565217391305 %\n"
     ]
    }
   ],
   "source": [
    "#Accuracy of classification for testing set\n",
    "print(\"Accuracy = \"+str(accuracy(Y_test,predictions))+ \" %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Database:  Arrhythmia\n",
    "\n",
    "#   Class code :   Class   :                       Number of instances:\n",
    "#   01             Normal                          245\n",
    "#   02             Ischemic changes (Coronary Artery Disease)   44\n",
    "#   03             Old Anterior Myocardial Infarction           15\n",
    "#   04             Old Inferior Myocardial Infarction           15\n",
    "#   05             Sinus tachycardy                     13\n",
    "#   06             Sinus bradycardy                     25\n",
    "#   07             Ventricular Premature Contraction (PVC)       3\n",
    "#   08             Supraventricular Premature Contraction        2\n",
    "#   09             Left bundle branch block                  9   \n",
    "#   10             Right bundle branch block                50\n",
    "#   11             1. degree AtrioVentricular block              0   \n",
    "#   12             2. degree AV block                    0\n",
    "#   13             3. degree AV block                    0\n",
    "#   14             Left ventricule hypertrophy               4\n",
    "#   15             Atrial Fibrillation or Flutter                5\n",
    "#   16             Others                           22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
